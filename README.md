# ğŸ¨ Fast Neural Style Transfer (PyTorch)

A **Fast Neural Style Transfer** implementation in **PyTorch** that generates stylized images in **real time** using a feed-forward neural network trained with **perceptual loss**.

Inspired by *Johnson et al., 2016*, this approach trains a single network per style to perform instant stylization, unlike optimization-based neural style transfer methods.

---

## âœ¨ Features

- ğŸš€ Real-time image stylization using a feed-forward CNN  
- ğŸ§  Perceptual content & style loss using pretrained **VGG-16**  
- ğŸ” Residual architecture with **Instance Normalization**  
- ğŸ–¼ï¸ Supports training and inference  
- ğŸ“¦ ONNX export for deployment  
- âš™ï¸ Clean and modular PyTorch codebase  

---

## ğŸ–¼ï¸ Results

Below are example outputs generated by the trained model.

### Example Outputs

<p align="center">
  <img src="images/Example%201.png" width="90%">
</p>

<p align="center">
  <img src="images/Example%202.png" width="90%">
</p>

<p align="center">
  <img src="images/App_ss_1.png" width="90%">
</p>

Each example shows the transformation of a content image into a stylized output using a single forward pass.

---

## ğŸ“‰ Training Loss Curve

The plot below shows the perceptual loss trend during training, demonstrating stable convergence.

<p align="center">
  <img src="images/extended_loss_plot.png" width="85%">
</p>

> âš ï¸ If this image does not load, rename the file to **extended_loss_plot.png**
> or update the filename here to match exactly.

---

## ğŸ—ï¸ Model Architecture

The style transfer network is a feed-forward **Transformer Network** composed of convolutional, residual, and upsampling layers.

<p align="center">
  <img src="images/Model_structure.png" width="85%">
</p>

### Architecture Overview

- Initial convolution layers with **Reflection Padding**
- Downsampling using strided convolutions
- **5 Residual Blocks** for feature transformation
- Upsampling via nearest-neighbor interpolation + convolution
- **Instance Normalization** for improved stylization quality

---

## ğŸ§  Perceptual Loss

Training is guided by **perceptual loss**, computed in feature space using a pretrained **VGG-16** network.

- **Content Loss**  
  Mean Squared Error between content and generated image features  

- **Style Loss**  
  Mean Squared Error between Gram matrices of style and generated image features  

This allows the model to preserve content structure while applying rich artistic styles.


---

## ğŸ“š References

* Johnson, J., Alahi, A., & Fei-Fei, L. *Perceptual Losses for Real-Time Style Transfer and Super-Resolution*, ECCV 2016
* Gatys, L. A., Ecker, A. S., & Bethge, M. *Image Style Transfer Using Convolutional Neural Networks*

---

## ğŸ™Œ Author

**Dhruv Garg**
Computer Vision & Deep Learning Enthusiast

---